<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>tidymodels</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr.¬†D‚ÄôAgostino McGowan" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# tidymodels
### Dr.¬†D‚ÄôAgostino McGowan

---







layout: true

&lt;div class="my-footer"&gt;
&lt;span&gt;
Dr. Lucy D'Agostino McGowan &lt;i&gt; adapted from Alison Hill's Introduction to ML with the Tidyverse&lt;/i&gt;
&lt;/span&gt;
&lt;/div&gt; 



---

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `tidymodels`

- Go to the [sta-363-s20 GitHub organization](https://github.com/sta-363-s20) and search for `appex-04-tidymodels`
- Go to RStudio Pro
- rstudio.hpc.ar53.wfu.edu:8787
- pw: R2D2Star!

---

## tidymodels

.pull-left[
![](img/02/tidymodels.png)
]

.pull-right[
.center[
[tidyverse.org](https://www.tidyverse.org/)
]

- tidymodels is an opinionated collection of R packages designed for modeling and statistical analysis.
- All packages share an underlying philosophy and a common grammar.
]

---

## Step 1: Specify the model

* Pick the **model**
--

* Set the **engine**

---

## Specify the model



```r
linear_reg() %&gt;%
  set_engine("lm")
```

---

## Specify the model



```r
linear_reg() %&gt;%
  set_engine("glmnet")
```

---

## Specify the model



```r
linear_reg() %&gt;%
  set_engine("spark")
```

---

## Specify the model



```r
decision_tree() %&gt;%
  set_engine("ranger")
```

---

## Specify the model

* All available models:

https://tidymodels.github.io/parsnip/articles/articles/Models.html

---

class: inverse

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Specify Model`

Write a pipe that creates a model that uses `lm()` to fit a linear regression using tidymodels. Save it as `lm_spec` and look at the object. What does it return?

_Hint: you'll need  https://tidymodels.github.io/parsnip/articles/articles/Models.html_

<div class="countdown" id="timer_5e5dc180" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---


```r
lm_spec &lt;- 
  linear_reg() %&gt;% # Pick linear regression
  set_engine(engine = "lm") # set engine
lm_spec
```

```
## Linear Regression Model Specification (regression)
## 
## Computational engine: lm
```

---

## Fit the data

* You can train your model using the `fit()` function


```r
fit(lm_spec,
    formula = mpg ~ horsepower,
    data = Auto)
```

```
## parsnip model object
## 
## Fit time:  4ms 
## 
## Call:
## stats::lm(formula = formula, data = data)
## 
## Coefficients:
## (Intercept)   horsepower  
##     39.9359      -0.1578
```

---

class: inverse

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Fit Model`

Fit the model:


```r
library(ISLR)
lm_fit &lt;- fit(lm_spec,
              formula = mpg ~ horsepower,
              data = Auto)
lm_fit
```

Does this give the same results as


```r
lm(mpg ~ horsepower, data = Auto)
```

<div class="countdown" id="timer_5e5dc0bb" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">30</span></code>
</div>

---




## Get predictions


```r
lm_fit %&gt;%
  predict(new_data = Auto)
```

--

* Still uses the `predict()` function
--

* ‚ÄºÔ∏è Now `new_data` has an underscore
--

* üòÑ This automagically creates a data frame

---

## Get predictions


```r
lm_fit %&gt;%
  predict(new_data = Auto) %&gt;%
  bind_cols(Auto)
```

```
## # A tibble: 392 x 10
##    .pred   mpg cylinders displacement horsepower weight acceleration  year
##    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;
##  1 19.4     18         8          307        130   3504         12      70
##  2 13.9     15         8          350        165   3693         11.5    70
##  3 16.3     18         8          318        150   3436         11      70
##  4 16.3     16         8          304        150   3433         12      70
##  5 17.8     17         8          302        140   3449         10.5    70
##  6  8.68    15         8          429        198   4341         10      70
##  7  5.21    14         8          454        220   4354          9      70
##  8  6.00    14         8          440        215   4312          8.5    70
##  9  4.42    14         8          455        225   4425         10      70
## 10  9.95    15         8          390        190   3850          8.5    70
## # ‚Ä¶ with 382 more rows, and 2 more variables: origin &lt;dbl&gt;, name &lt;fct&gt;
```

---

class: inverse

<div class="countdown" id="timer_5e5dc251" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">30</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Get predictions`

Edit the code below to add the original data to the predicted data.



```r
mpg_pred &lt;- lm_fit %&gt;% 
  predict(new_data = Auto) %&gt;% 
  ---
```

---

## Get predictions


```r
mpg_pred &lt;- lm_fit %&gt;%
  predict(new_data = Auto) %&gt;%
  bind_cols(Auto)

mpg_pred
```

```
## # A tibble: 392 x 10
##    .pred   mpg cylinders displacement horsepower weight acceleration  year
##    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;
##  1 19.4     18         8          307        130   3504         12      70
##  2 13.9     15         8          350        165   3693         11.5    70
##  3 16.3     18         8          318        150   3436         11      70
##  4 16.3     16         8          304        150   3433         12      70
##  5 17.8     17         8          302        140   3449         10.5    70
##  6  8.68    15         8          429        198   4341         10      70
##  7  5.21    14         8          454        220   4354          9      70
##  8  6.00    14         8          440        215   4312          8.5    70
##  9  4.42    14         8          455        225   4425         10      70
## 10  9.95    15         8          390        190   3850          8.5    70
## # ‚Ä¶ with 382 more rows, and 2 more variables: origin &lt;dbl&gt;, name &lt;fct&gt;
```

---

## Calculate the error

* Root mean square error


```r
mpg_pred %&gt;%
  rmse(truth = mpg, estimate = .pred)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard        4.89
```

--

.question[
What is this estimate? (training error? testing error?)
]

---

## Validation set approach


```r
Auto_split &lt;- initial_split(Auto, prop = 0.5)
Auto_split
```

```
## &lt;196/196/392&gt;
```

--

* Extract the training and testing data


```r
training(Auto_split)
testing(Auto_split)
```

---

## Validation set approach


```r
Auto_train &lt;- training(Auto_split)
```


```r
Auto_train
```

.small[

```
## # A tibble: 196 x 9
##      mpg cylinders displacement horsepower weight acceleration  year origin
##    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1    18         8          307        130   3504         12      70      1
##  2    18         8          318        150   3436         11      70      1
##  3    17         8          302        140   3449         10.5    70      1
##  4    14         8          454        220   4354          9      70      1
##  5    15         8          383        170   3563         10      70      1
##  6    14         8          340        160   3609          8      70      1
##  7    15         8          400        150   3761          9.5    70      1
##  8    14         8          455        225   3086         10      70      1
##  9    24         4          113         95   2372         15      70      3
## 10    18         6          199         97   2774         15.5    70      1
## # ‚Ä¶ with 186 more rows, and 1 more variable: name &lt;fct&gt;
```
]


---

class: inverse

<div class="countdown" id="timer_5e5dc004" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Validation Set`

Copy the code below, fill in the blanks to fit a model on the **training** data then calculate the **test** RMSE.


```r
set.seed(100)
Auto_split  &lt;- ________
Auto_train  &lt;- ________
Auto_test   &lt;- ________
lm_fit      &lt;- fit(lm_spec, 
                   formula = mpg ~ horsepower, 
                   data = ________)
mpg_pred  &lt;- ________ %&gt;% 
  predict(new_data = ________) %&gt;% 
  bind_cols(________)
rmse(________, truth = ________, estimate = ________)
```

---

## What about cross validation?


```r
Auto_cv &lt;- vfold_cv(Auto, v = 5)
Auto_cv
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 2
##   splits           id   
##   &lt;named list&gt;     &lt;chr&gt;
## 1 &lt;split [313/79]&gt; Fold1
## 2 &lt;split [313/79]&gt; Fold2
## 3 &lt;split [314/78]&gt; Fold3
## 4 &lt;split [314/78]&gt; Fold4
## 5 &lt;split [314/78]&gt; Fold5
```

---

## What about cross validation?

--


```r
fit_resamples(lm_spec,
              mpg ~ horsepower,
              resamples = Auto_cv)
```

---

## What about cross validation?


```r
fit_resamples(lm_spec,
              mpg ~ horsepower,
              resamples = Auto_cv)
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 4
##   splits           id    .metrics         .notes          
## * &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          
## 1 &lt;split [313/79]&gt; Fold1 &lt;tibble [2 √ó 3]&gt; &lt;tibble [0 √ó 1]&gt;
## 2 &lt;split [313/79]&gt; Fold2 &lt;tibble [2 √ó 3]&gt; &lt;tibble [0 √ó 1]&gt;
## 3 &lt;split [314/78]&gt; Fold3 &lt;tibble [2 √ó 3]&gt; &lt;tibble [0 √ó 1]&gt;
## 4 &lt;split [314/78]&gt; Fold4 &lt;tibble [2 √ó 3]&gt; &lt;tibble [0 √ó 1]&gt;
## 5 &lt;split [314/78]&gt; Fold5 &lt;tibble [2 √ó 3]&gt; &lt;tibble [0 √ó 1]&gt;
```

---

## What about cross validation?

.question[
How do we get the metrics out? With `collect_metrics()`!
]

--


```r
results &lt;- fit_resamples(lm_spec,
                         mpg ~ horsepower,
                         resamples = Auto_cv)

results %&gt;%
  collect_metrics()
```

```
## # A tibble: 2 x 5
##   .metric .estimator  mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   4.91      5  0.180 
## 2 rsq     standard   0.613     5  0.0193
```

---


class: inverse

<div class="countdown" id="timer_5e5dbf42" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `K-fold cross validation`


Edit the code below to get the 5-fold cross validation error rate for the following model:

`\(mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2+ \epsilon\)`


```r
Auto_cv &lt;- vfold_cv(Auto, v = 5)

results &lt;- fit_resamples(lm_spec,
                         ----,
                         resamples = ---)

results %&gt;%
  collect_metrics()
```

* What do you think `rsq` is?

---

## What if we wanted to do some _preprocessing_

* For the shrinkage methods we discussed it was important to _scale_ the variables

--

.question[
What does this mean?
]

--

.question[
What would happen if we _scale_ **before** doing cross-validation? Will we get different answers?
]

---

## What if we wanted to do some _preprocessing_

.small[

```r
Auto_scaled &lt;- Auto %&gt;%
  mutate(horsepower = scale(horsepower))

sd(Auto_scaled$horsepower)
```

```
## [1] 1
```



```r
Auto_cv_scaled &lt;- vfold_cv(Auto_scaled, v = 5)

map_dbl(Auto_cv_scaled$splits,
        function(x) {
          dat &lt;- as.data.frame(x)$horsepower
          sd(dat)
        })
```

```
##         1         2         3         4         5 
## 1.0352173 1.0093796 0.9721922 0.9868816 0.9948661
```

]

---

## What if we wanted to do some _preprocessing_

* `recipe()`!
--

* Using the `recipe()` function along with `step_*()` functions, we can specify _preprocessing_ steps and R will automagically apply them to each fold appropriately.
--


```r
rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
* step_scale(horsepower)
```

--

* You can find all of the potential preprocessing **steps** here: https://tidymodels.github.io/recipes/reference/index.html

---

## Where do we plug in this recipe?

* The `recipe` gets plugged into the `fit_resamples()` function

--


```r
Auto_cv &lt;- vfold_cv(Auto, v = 5)

rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
  step_scale(horsepower)

results &lt;- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)

results %&gt;%
  collect_metrics()
```

```
## # A tibble: 2 x 5
##   .metric .estimator  mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   4.91      5  0.208 
## 2 rsq     standard   0.608     5  0.0279
```

---
class: inverse

<div class="countdown" id="timer_5e5dc209" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Pre-processing`

* Look through the available steps here: https://tidymodels.github.io/recipes/reference/index.html
* See if you can find a way to make `horsepower` a 3rd degree polynomial (like in the lab) 
* Edit the code below to run this model


```r
rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
  step_---(horsepower, ---)

results &lt;- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)

results %&gt;%
  collect_metrics()
```

---

## Recipe!


```r
rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
  step_poly(horsepower, degree = 3)

results &lt;- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)

results %&gt;%
  collect_metrics()
```

```
## # A tibble: 2 x 5
##   .metric .estimator  mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   4.39      5  0.241 
## 2 rsq     standard   0.681     5  0.0279
```

---

## Okay, but we wanted to look at 3 different models!

.small[

```r
rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
  step_poly(horsepower, degree = 1)

results &lt;- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)
```
]

--

.small[

```r
rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
  step_poly(horsepower, degree = 2)

results &lt;- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)
```
]

---

.small[

```r
rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
  step_poly(horsepower, degree = 3)

results &lt;- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)
```
]

--

* üò± this looks like copy + pasting!

---

## tune üé∂

* First, create a recipe with pre-processing steps


```r
rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
* step_poly(horsepower, degree = tune())
```

--
* Notice the code above has `tune()` for the degree of the polynomial. That is the thing we want to vary!

---

##  tune üé∂

* Now we need to create a grid of potential degrees of the polynomial that we want to test
* Instead of `fit_resamples()` we are going to use `tune_grid()`


```r
grid &lt;- expand_grid(degree = c(1, 2, 3))

results &lt;- tune_grid(lm_spec,
                     preprocessor = rec,
*                    grid = grid,
                     resamples = Auto_cv)
```

---

## tune üé∂


```r
results %&gt;%
  collect_metrics()
```

```
## # A tibble: 6 x 6
##   degree .metric .estimator  mean     n std_err
##    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1      1 rmse    standard   4.91      5  0.208 
## 2      1 rsq     standard   0.608     5  0.0279
## 3      2 rmse    standard   4.37      5  0.236 
## 4      2 rsq     standard   0.684     5  0.0273
## 5      3 rmse    standard   4.39      5  0.241 
## 6      3 rsq     standard   0.681     5  0.0279
```

---

## Subset results


```r
results %&gt;%
  collect_metrics() %&gt;%
  filter(.metric == "rmse")
```

```
## # A tibble: 3 x 6
##   degree .metric .estimator  mean     n std_err
##    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1      1 rmse    standard    4.91     5   0.208
## 2      2 rmse    standard    4.37     5   0.236
## 3      3 rmse    standard    4.39     5   0.241
```

* Since this is a data frame, we can do things like filter!
--

.question[
Which would you choose?
]

---

##  tune üé∂

* We can easily tweak this process to look at more polynomial degrees by adding more numbers to our `grid` data frame.

---

class: inverse

<div class="countdown" id="timer_5e5dc1ac" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `K-fold cross validation`

Edit the code below to look at 1-10 degree polynomials. **Bonus** Try to plot the RMSE by degree


```r
rec &lt;- recipe(mpg ~ horsepower, data = Auto) %&gt;%
  step_poly(---) 

grid &lt;- expand_grid(degree = ---)

results &lt;- tune_grid(lm_spec,
                     preprocessor = ---,
                     grid = ---,
                     resamples = ---)

results %&gt;%
  collect_metrics()
```


---



```r
results %&gt;%
  collect_metrics() %&gt;%
  filter(.metric == "rmse") %&gt;%
  ggplot(aes(degree, mean)) + 
  geom_point() + 
  labs(y = RMSE)
```

![](12-tidymodels_files/figure-html/rmse-plot-1.png)&lt;!-- --&gt;

---

## Ridge, Lasso, and Elastic net

* When specifying your model, you can indicate whether you would like to use ridge, lasso, or elastic net. We can write a general equation to minimize:

`$$RSS + \lambda\left((1-\alpha)\sum_{i=1}^p\beta_j^2+\alpha\sum_{i=1}^p|\beta_j|\right)$$`

--


```r
lm_spec &lt;- linear_reg() %&gt;%
* set_engine("glmnet")
```

* First specify the engine. We'll use `glmnet`
--

* The `linear_reg()` function has two additional parameters, `penalty` and `mixture`
--

* `penalty` is `\(\lambda\)` from our equation. 
--

* `mixture` is a number between 0 and 1 representing `\(\alpha\)`

---


## Ridge, Lasso, and Elastic net


`$$RSS + \lambda\left((1-\alpha)\sum_{i=1}^p\beta_j^2+\alpha\sum_{i=1}^p|\beta_j|\right)$$`


.question[
What would we set `mixture` to in order to perform Ridge regression?
]

--

.small[

```r
*ridge_spec &lt;- linear_reg(penalty = 100, mixture = 0) %&gt;%
  set_engine("glmnet") 
```
]

---


class: inverse

<div class="countdown" id="timer_5e5dc239" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Lasso specification`

Set up the model specification to fit a Lasso with a `\(\lambda\)` value of 5. Call this object `lasso_spec`.


---

## Ridge, Lasso, and Elastic net


`$$RSS + \lambda\left((1-\alpha)\sum_{i=1}^p\beta_j^2+\alpha\sum_{i=1}^p|\beta_j|\right)$$`

.small[

```r
*ridge_spec &lt;- linear_reg(penalty = 100, mixture = 0) %&gt;%
  set_engine("glmnet") 
```
]

--

.small[

```r
*lasso_spec &lt;- linear_reg(penalty = 5, mixture = 1) %&gt;%
  set_engine("glmnet") 
```
]

--

.small[

```r
*enet_spec &lt;- linear_reg(penalty = 60, mixture = 0.7) %&gt;%
  set_engine("glmnet") 
```
]

---

## Motivating example

We want to predict a baseball player's `Salary` on the basis of various statistics
associated with performance in the previous year.

* We will use the `Hitters` data frame from the **ISLR** package
--

* This data frame has some missing values. For simplicity, we are going to remove all missing rows (do a **complete case** analysis) using:

.small[

```r
hitters &lt;- na.omit(Hitters)
```
]

_If you do this in practice, be sure to state that explictly_.

---

class: inverse

<div class="countdown" id="timer_5e5dc21e" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Baseball prediction, CV set up`

Set up the 5-fold cross validation by creating an object called `hitters_cv`


```r
library(ISLR)
hitters &lt;- na.omit(Hitters)
set.seed(1)

hitters_cv &lt;- -----(hitters, -----)
```

---

## Baseball prediction cross validation


```r
set.seed(1)
hitters_cv &lt;- vfold_cv(hitters, v = 5)
```

---

## Baseball prediction preprocessing

.question[
How does regression handle categorical (nominal) predictors?
]

--

* Creates "dummy" variables

--

.small[

```
##      Species
## 1     setosa
## 2     setosa
## 3 versicolor
## 4  virginica
```

]

--

.small[

```
## # A tibble: 4 x 2
##   Species_versicolor Species_virginica
##                &lt;dbl&gt;             &lt;dbl&gt;
## 1                  0                 0
## 2                  0                 0
## 3                  1                 0
## 4                  0                 1
```
]


--

---

## Baseball prediction preprocessing

* The first preprocessing step we want to do is make all nominal variables **dummy variables**.
* You can do this by specifying `step_dummy()` in a `recipe`
--

* Specify the variables you want to be preprocessed by seperating them with commas:


```r
step_dummy(nominal_variable_1, nominal_variable_2, ...)
```

---

class: inverse

<div class="countdown" id="timer_5e5dc0e7" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Baseball prediction, preprocessing`

For this prediction model, we want to predict `Salary` using **all** remaining variables (`Salary ~ .`). Examine the `hitters` data. Find the **nominal variables**. Ammend the code below to create dummy variables for these.


```r
rec &lt;- recipe(Salary ~ ., data = hitters) %&gt;%
  step_dummy(------)
```

---

## Baseball prediction preprocessing

_There is an easier way!_ 

--

* Instead of looking for all nominal variables by hand, you can use `all_nominal()`.
--


```r
rec &lt;- recipe(Salary ~ ., data = hitters) %&gt;%
  step_dummy(all_nominal())
```

---

## Baseball prediction preprocessing

.question[
What pre-processing step is necessary when doing penalized regression?
]

--

* Instead of plugging each variable individually into `step_scale()`, you can use the shorthand `all_predictors()`:

.small[

```r
step_scale(all_predictors())
```
]


---

class: inverse

<div class="countdown" id="timer_5e5dbf6e" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Baseball prediction, preprocessing`

Ammend the code below to make all nominal variables dummy variables and scale all of the predictors


```r
rec &lt;- recipe(Salary ~ ., data = hitters) %&gt;%
  step_-----(------) %&gt;%
  step_-----(------)
```

---

## Baseball prediction preprocessing


```r
rec &lt;- recipe(Salary ~ ., data = hitters) %&gt;%
  step_dummy(all_nominal()) %&gt;%
  step_scale(all_predictors())
```

---


class: inverse

<div class="countdown" id="timer_5e5dc16b" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Baseball prediction, lasso`

Using the recipe you've created `rec`, the cross validation data `hitters_cv`, and the `lasso_spec` fit the resampled data. Output the `rmse` and `rsq` for this model. 


```r
results &lt;- fit_resamples(lasso_spec,
                         preprocessor = ---,
                         resamples = ---)

results %&gt;%
  ---
```

---

## Ridge, Lasso, and Elastic Net

`$$RSS + \lambda\left((1-\alpha)\sum_{i=1}^p\beta_j^2+\alpha\sum_{i=1}^p|\beta_j|\right)$$`

.question[
What if I don't want to specify these parameters? How can I specify that I want to use cross validation to choose?
]

--

* üé∂

--


```r
penalized_spec &lt;- 
  linear_reg(penalty = tune(),
             mixture = tune()) %&gt;%
  set_engine("glmnet")

grid &lt;- expand_grid(penalty = seq(1, 1000, by = 100),
                         mixture = seq(0, 1, by = 0.2))
```

---

class: inverse

<div class="countdown" id="timer_5e5dc16c" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Baseball prediction, tuned penalization`

Create a new models specification called `penalized_spec` that tunes the penalty and mixture parameters. Create a grid of potential parameters, with a penalty term that varies from 0 to 100 by 10, and a mixture term that varies from 0 to 1 by 0.1. Using the recipe you've created `rec`and the cross validation data `hitters_cv` fit this model using `tune_grid`. Output the `rmse` and `rsq` for this model. 

.small[

```r
penalized_spec &lt;- linear_reg(penalty = ---,
                             mixture = ---) %&gt;%
                  set_engine("glmnet")

grid &lt;- expand_grid(penalty = ---,
                    mixture = ---)

results &lt;- tune_grid(penalized_spec,
                     preprocessor = ---,
                     grid = ---,
                     resamples = ---)

results %&gt;%
  ---
```
]

---

## Baseball prediction, tuned penalization



```r
penalized_spec &lt;- 
  linear_reg(penalty = tune(),
             mixture = tune()) %&gt;%
  set_engine("glmnet")

grid &lt;- expand_grid(penalty = seq(0, 100, by = 10),
                         mixture = seq(0, 1, by = 0.1))

results &lt;- tune_grid(penalized_spec,
                     rec,
                     grid = grid,
                     resamples = hitters_cv)
```

---


class: inverse

<div class="countdown" id="timer_5e5dc031" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

## &lt;i class="fas fa-laptop"&gt;&lt;/i&gt; `Baseball prediction, choosing penalty and mixture`

Examine the results. Filter the metrics to only include `rmse` and arrange them in order of mean root mean squared error (to choose the smallest). Hint look at `?arrange` if you aren't sure how to sort.


```r
results %&gt;%
  collect_metrics() %&gt;%
  filter(---) %&gt;%
  arrange(---)
```

---

## Baseball prediction tuning


```r
results %&gt;%
  collect_metrics() %&gt;%
  filter(.metric == "rmse") %&gt;%
  arrange(mean)
```

```
## # A tibble: 121 x 7
##    penalty mixture .metric .estimator  mean     n std_err
##      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1       0     0   rmse    standard    344.     5    24.1
##  2      10     0   rmse    standard    344.     5    24.1
##  3      20     0   rmse    standard    344.     5    24.1
##  4     100     0   rmse    standard    344.     5    23.7
##  5      90     0   rmse    standard    344.     5    23.8
##  6      80     0   rmse    standard    344.     5    23.9
##  7      70     0   rmse    standard    344.     5    24.0
##  8      10     0.1 rmse    standard    344.     5    23.4
##  9      30     0   rmse    standard    344.     5    24.1
## 10      60     0   rmse    standard    344.     5    24.1
## # ‚Ä¶ with 111 more rows
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
