---
title: "Logistic regression, LDA, QDA"
author: "Dr. D'Agostino McGowan"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r child = "setup.Rmd"}
```

layout: true

<div class="my-footer">
<span>
Dr. Lucy D'Agostino McGowan <i>adapted from slides by Hastie & Tibshirani</i>
</span>
</div> 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(broom)
library(gridExtra)
library(ISLR)
```

---

## `r emo::ji("point_up")` Reminders

* Homework 1 is due tomorrow (be sure to **knit, commit, push**)
* Study sessions are 7-9 in Manchester 122
* Questions? Use [github.com/sta-363-s20/community](https://github.com/sta-363-s20/community)

---

class: center, middle

## `r emo::ji("book")` Canvas

* _use Google Chrome_

---

## <i class="fas fa-laptop"></i> `LDA`

- Go to the [sta-363-s20 GitHub organization](https://github.com/sta-363-s20) and search for `appex-02-lda`
- Clone this repository into RStudio Cloud

---

## Recap

* Last class we had a _linear regression_ refresher
--

* We covered how to write a linear model in _matrix_ form
--

* We learned how to minimize RSS to calculate $\hat{\beta}$ with $(\mathbf{X^TX)^{-1}X^Ty}$
--

* Linear regression is a great tool when we have a continuous outcome
* We are going to learn some fancy ways to do even better in the future
---

class: center, middle

# Classification

---

## Classification

.question[
What are some examples of classification problems?
]
* Qualitative response variable in an _unordered set_, $\mathcal{C}$  
--

  * `eye color` $\in$ `{blue, brown, green}`
  * `email` $\in$ `{spam, not spam}`
--

* Response, $Y$ takes on values in $\mathcal{C}$
* Predictors are a vector, $X$
--

* The task: build a function $C(X)$ that takes $X$ and predicts $Y$, $C(X)\in\mathcal{C}$ 
--

* Many times we are actually more interested in the _probabilities_ that $X$ belongs to each category in $\mathcal{C}$

---

## Example: Credit card default

```{r plot1, cache = TRUE, fig.height = 3.5}
set.seed(1)
Default %>%
  sample_frac(size = 0.25) %>%
  ggplot(aes(balance, income, color = default)) +
  geom_point(pch = 4) +
  scale_color_manual(values = c("cornflower blue", "red")) +
  theme_classic() +
  theme(legend.position = "top") -> p1

p2 <- ggplot(Default, aes(x = default, y = balance, fill = default)) +
  geom_boxplot() +
  scale_fill_manual(values = c("cornflower blue", "red")) +
  theme_classic() +
  theme(legend.position = "none")

p3 <- ggplot(Default, aes(x = default, y = income, fill = default)) +
  geom_boxplot() +
  scale_fill_manual(values = c("cornflower blue", "red")) +
  theme_classic() +
  theme(legend.position = "none")
grid.arrange(p1, p2, p3, ncol = 3, widths = c(2, 1, 1))
```

---

## Can we use linear regression?

We can code `Default` as

$$Y = \begin{cases} 0 & \textrm{if }\texttt{No}\\ 1&\textrm{if }\texttt{Yes}\end{cases}$$
Can we fit a linear regression of $Y$ on $X$ and classify as `Yes` if $\hat{Y}> 0.5$?

--

* In this case of a **binary** outcome, linear regression is okay (it is equivalent to **linear discriminant analysis**, we'll get to that soon!)
* $E[Y|X=x] = P(Y=1|X=x)$, so it seems like this is a pretty good idea!
* **The problem**: Linear regression can produce probabilities less than 0 or greater than 1 `r emo::ji("scream")`
--
.question[
What may do a better job?
]
--

* **Logistic regression!**

---

## Linear versus logistic regression

```{r plot2, cache = TRUE}
Default <- Default %>%
  mutate(
  p = glm(default ~ balance, data = Default, family = "binomial") %>%
  predict(type = "response"),
  p2 = lm(I(default == "Yes") ~ balance, data = Default) %>% predict(),
  def = ifelse(default == "Yes", 1, 0)
)


Default %>%
  sample_frac(0.25) %>%
ggplot(aes(balance, p2)) +
  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +
  geom_line(color = "cornflower blue") +
  geom_point(aes(balance, def), shape = "|", color = "orange") +
  theme_classic() +
  labs(y = "probability of default") -> p1

Default %>%
  sample_frac(0.25) %>%
ggplot(aes(balance, p)) +
  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +
  geom_line(color = "cornflower blue") +
  geom_point(aes(balance, def), shape = "|", color = "orange") +
  theme_classic() +
  labs(y = "probability of default") -> p2

grid.arrange(p1, p2, ncol = 2)
```

.question[
Which does a better job at predicting the probability of default?
]

* The orange marks represent the response $Y\in\{0,1\}$

---

## Linear Regression

What if we have $>2$ possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms

$$ 
\begin{align}
Y = \begin{cases} 1& \textrm{if }\texttt{stroke}\\2&\textrm{if }\texttt{drug overdose}\\3&\textrm{if }\texttt{epileptic seizure}\end{cases}
\end{align}
$$

.question[
What could go wrong here?
]
--

* The coding implies an _ordering_
* The coding implies _equal spacing_ (that is the difference between `stroke` and `drug overdose` is the same as `drug overdose` and `epileptic seizure`)
---

## Linear Regression

What if we have $>2$ possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms

$$ 
\begin{align}
Y = \begin{cases} 1& \textrm{if }\texttt{stroke}\\2&\textrm{if }\texttt{drug overdose}\\3&\textrm{if }\texttt{epileptic seizure}\end{cases}
\end{align}
$$

* Linear regression is **not** appropriate here
* **Mutliclass logistic regression** or **discriminant analysis** are more appropriate

---

## Logistic Regression

$$ 
p(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}
$$

* Note: $p(X)$ is shorthand for $P(Y=1|X)$
* No matter what values $\beta_0$, $\beta_1$, or $X$ take $p(X)$ will always be between 0 and 1
--

* We can rearrange this into the following form:
$$
\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + \beta_1 X
$$

.question[
What is this transformation called?
]
--

* This is a **log odds** or **logit** transformation of $p(X)$

---

## Linear versus logistic regression

```{r}
grid.arrange(p1, p2, ncol = 2)
```

Logistic regression ensures that our estimates for $p(X)$ are between 0 and 1 `r emo::ji("tada")`

---

## Maximum Likelihood

.question[
Refresher: How did we estimate $\hat\beta$ in linear regression?

]

--
In logistic regression, we use **maximum likelihood** to estimate the parameters

$$\mathcal{l}(\beta_0,\beta_1)=\prod_{i:y_i=1}p(x_i)\prod_{i:y_i=0}(1-p(x_i))$$
--

* This **likelihood** give the probability of the observed ones and zeros in the data
* We pick $\beta_0$ and $\beta_1$ to maximize the likelihood
* _We'll let `R` do the heavy lifting here_

---

## Let's see it in R

.small[
```{r, echo = TRUE}
glm(default ~ balance, data = Default, family = "binomial") %>%
  tidy()
```
]

* Use the `glm()` function in R with the `family = "binomial"` argument

---

## Making predictions

.question[
What is our estimated probability of default for someone with a balance of $1000?
]

```{r}
glm(default ~ balance, data = Default, family = "binomial") %>%
  tidy() %>%
  knitr::kable(format = "html")
```

--

$$
\hat{p}(X) = \frac{e^{\hat{\beta}_0+\hat{\beta}_1}X}{1+e^{\hat{\beta}_0+\hat{\beta}_1X}}=\frac{e^{-10.65+0.0055\times 1000}}{1+e^{-10.65+0.0055\times 1000}}=0.006
$$

---

## Making predictions

.question[
What is our estimated probability of default for someone with a balance of $2000?
]

```{r}
glm(default ~ balance, data = Default, family = "binomial") %>%
  tidy() %>%
  knitr::kable(format = "html")
```

--

$$
\hat{p}(X) = \frac{e^{\hat{\beta}_0+\hat{\beta}_1}X}{1+e^{\hat{\beta}_0+\hat{\beta}_1X}}=\frac{e^{-10.65+0.0055\times 2000}}{1+e^{-10.65+0.0055\times 2000}}=0.586
$$

---

## Logistic regression example

Let's refit the model to predict the probability of default given the customer is a `student`

```{r}
glm(default ~ student, data = Default, family = "binomial") %>%
  tidy() %>%
  knitr::kable(format = "html")
```

$$P(\texttt{default = Yes}|\texttt{student = Yes}) = \frac{e^{-3.5041+0.4049\times1}}{1+e^{-3.5041+0.4049\times1}}=0.0.0431$$
--

.question[
How will this change if `student = No`?
]

--

$$P(\texttt{default = Yes}|\texttt{student = No}) = \frac{e^{-3.5041+0.4049\times0}}{1+e^{-3.5041+0.4049\times0}}=0.0.0292$$

---

## Multiple logistic regression

$$\log\left(\frac{p(X)}{1-p(X)}\right)=\beta_0+\beta_1X_1+\dots+\beta_pX_p$$
$$p(X) = \frac{e^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}}$$

```{r}
glm(default ~ balance + income + student, data = Default, family = "binomial") %>%
  tidy() %>%
  knitr::kable(format = "html")
```

--
* Why is the coefficient for `student` negative now when it was positive before?

---

## Confounding

```{r plot3, cache = TRUE, fig.height = 2.25}
Default %>%
  mutate(balance_bin = case_when(
    balance < 250 ~ 250,
    balance < 500 ~ 500,
    balance < 750 ~ 750,
    balance < 1000 ~ 1000,
    balance < 1250 ~ 1250,
    balance < 1500 ~ 1500,
    balance < 1750 ~ 1750,
    balance < 2000 ~ 2000,
    balance < 2250 ~ 2250,
    TRUE ~ 2500
  )) %>%
  group_by(balance_bin, student) %>%
  summarise(default_rate = mean(default == "Yes")) %>%
  ggplot(aes(balance_bin, default_rate, color = student)) + 
  geom_line() + 
  scale_color_manual(values = c("cornflower blue", "orange")) + 
  geom_hline(yintercept = c(mean(Default$default[Default$student == "Yes"] == "Yes"), mean(Default$default[Default$student == "No"] == "Yes")), color = c("orange", "cornflower blue"), lty = 2) +
  xlim(c(500, 2250)) + 
  labs(y = "Default rate", x = "Credit card balance") +
  theme_classic() + 
  theme(legend.position = "none") -> p1

ggplot(Default, aes(student, balance, fill = student)) +
  geom_boxplot() + 
  scale_fill_manual(values = c("cornflower blue", "orange")) + 
  theme_classic() + 
  labs(y = "Credit card balance", x = "Student status") -> p2

grid.arrange(p1, p2, ncol = 2)
  
```

.question[
What is going on here?
]
---

## Confounding

```{r, fig.height = 2.25}
grid.arrange(p1, p2, ncol = 2)
```

* Students tend to have higher balances than non-students
  * Their **margignal** default rate is higher
--

* For each level of balance, students default less 
  * Their **conditional** default rate is lower

---

## Logistic regression for more than two classes

* So far we've discussed **binary** outcome data
* We can generalize this to situations with **multiple** classes

$$P(Y=k|X) = \frac{e ^{\beta_{0k}+\beta_{1k}X_1+\dots+\beta_{pk}X_p}}{\sum_{l=1}^Ke^{\beta_{0l}+\beta_{1l}X_1+\dots+\beta_{pl}X_p}}$$

* Here we have a linear function for **each** of the $K$ classes
* This is known as **multinomial logistic regression**

---

## Discriminant Analysis

* Another way to model multiple classes
`r emo::ji("bulb")` Big idea: 
* Model the distribution of $X$ in each class separately ( $P(X|Y)$ )
* Use **Bayes theorem** to flip things around to get $P(Y|X)$

---

## Bayes Theorem

.question[
What is Bayes theorem?
]

![](img/05/bayes.gif)

---

## Bayes Theorem

.question[
What is Bayes theorem?
]

$$P(Y=k|X=x) =\frac{P(X=x|Y=k)\times P(Y=k)}{P(X=x)}$$


![](img/05/bayes.gif)

---

## Bayes Theorem

$$\Large P(Y=k|X=x) =\frac{P(X=x|Y=k)\times P(Y=k)}{P(X=x)}$$

---

## Bayes Theorem

$$\Large \underbrace{P(Y=k|X=x)}_{posterior} =\frac{P(X=x|Y=k)\times P(Y=k)}{P(X=x)}$$

---

## Bayes Theorem

$$\Large P(Y=k|X=x) =\frac{\overbrace{P(X=x|Y=k)}^{likelihood}\times P(Y=k)}{P(X=x)}$$
---

## Bayes Theorem

$$\Large P(Y=k|X=x) =\frac{\overbrace{P(X=x|Y=k)}^{likelihood}\times \overbrace{P(Y=k)}^{prior}}{P(X=x)}$$

---

## Bayes Theorem Example

$$
\begin{align}
P(Sick|+)&=\frac{P(+|Sick)P(Sick)}{P(+)}\\
&=\frac{P(+|Sick)P(Sick)}{P(+|Sick)P(Sick)+P(+|Healthy)P(Healthy)}
\end{align}
$$
--

* Often when a test is _created_ the _sensitivity_ is calculated, that is the _true positive rate_, the $P(+|Sick)$. **Let's say in this case that is 99%**
--

* Let's suppose the probability of a **positive test if you are healthy is rare, 1%**
--

* Finally, let's suppose **the disease is fairly common, 20% of people in the population have it.**
--

.question[
What is my probability of having the disease given I tested positive?
]
---

## Bayes Theorem Example

$$
\begin{align}
P(Sick|+)&=\frac{P(+|Sick)P(Sick)}{P(+)}\\
.71&=\frac{0.99\times0.2}{0.99\times0.2+0.01\times0.8}
\end{align}
$$
* Often when a test is _created_ the _sensitivity_ is calculated, that is the _true positive rate_, the $P(+|Sick)$. **Let's say in this case that is 99%**
* Let's suppose the probability of a **positive test if you are healthy is rare, 1%**
* Finally, let's suppose **the disease is fairly common, 20% of people in the population have it.**

.question[
What is my probability of having the disease given I tested positive?
]
---

## Bayes Theorem Example

$$
\begin{align}
P(Sick|+)&=\frac{P(+|Sick)P(Sick)}{P(+)}\\
.71&=\frac{0.99\times0.2}{0.99\times0.2+0.01\times0.8}
\end{align}
$$
* Often when a test is _created_ the _sensitivity_ is calculated, that is the _true positive rate_, the $P(+|Sick)$. **Let's say in this case that is 99%**
* Let's suppose the probability of a **positive test if you are healthy is rare, 1%**
* If the disease is **rare (let's say 0.1% have it)** how does that change my probability of having it given a postive test?

.question[
What is my probability of having the disease given I tested positive?
]
---

## Bayes Theorem Example

$$
\begin{align}
P(Sick|+)&=\frac{P(+|Sick)P(Sick)}{P(+)}\\
0.009&=\frac{0.99\times0.001}{0.99\times0.001+0.01\times0.999}
\end{align}
$$
* Often when a test is _created_ the _sensitivity_ is calculated, that is the _true positive rate_, the $P(+|Sick)$. **Let's say in this case that is 99%**
* Let's suppose the probability of a **positive test if you are healthy is rare, 1%**
* If the disease is **rare (let's say 0.1% have it)** how does that change my probability of having it given a postive test?

.question[
What is my probability of having the disease given I tested positive?
]
---

## Bayes Theorem and Discriminant Analysis

$$P(Y=k|X=x) =\frac{P(X=x|Y=k)\times P(Y=k)}{P(X=x)}$$

This same equation is used for discriminant analysis with slightly different notation:
--

$$P(Y=k|X=x) =\frac{\pi_kf_k(x)}{\sum_{l=1}^Kf_l(x)}$$

--

* $f_k(x)=P(X=x|Y=k)$ is the **density** for $X$ in class $k$
  * For **linear discriminent analysis** we will use the **normal distribution** to represent this density
--

* $\pi_k=P(Y=k)$ is the marginal or **prior** probability for
class $k$

---

## Discriminant analysis

![](img/05/lda.png)

--

* Here there are two classes
--

* We classify new points based on which density is highest
--

* On the left, the priors for the two classes are the same
--

* On the right, we favor the pink class, making the decision boundary shift to the left

---

## Why discriminant analysis?

* When the classes are well separated, logistic regression is unstable, linear discriminant analysis (LDA) is not
--

* When $n$ is small and the distribution of predictors ( $X$ ) is approximately normal in each class, the linear discriminant model is more stable than the logistic model
--

* When we have more than 2 classes, LDA also provides a nice low dimensional way to **visualize** data

---

## Linear Discriminant Analysis p = 1

The density for the normal distribution is

$$f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k}e^{-\frac{1}{2}\left(\frac{x-\mu_k}{\sigma_k}\right)^2}$$
--

* $\mu_k$ is the mean in class $k$
--

* $\sigma^2_k$ is the variance $k$ (We will assume $\sigma_k=\sigma$ are the same for all classes)

---

## Linear Discriminant Analysis p = 1

The density for the normal distribution is

$$f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k}e^{-\frac{1}{2}\left(\frac{x-\mu_k}{\sigma_k}\right)^2}$$

* We can plug this into Bayes formula

$$p_k(X) =\frac{\pi_k\frac{1}{\sqrt{2\pi}\sigma_k}e^{-\frac{1}{2}\left(\frac{x-\mu_k}{\sigma_k}\right)^2}}{\sum_{l=1}^k\pi_l\frac{1}{\sqrt{2\pi}\sigma_l}e^{-\frac{1}{2}\left(\frac{x-\mu_l}{\sigma_l}\right)^2}}$$

--

`r emo::ji("sweat_smile")` Luckily things cancel!

---

## Discriminant functions

* To classify an observation where $X=x$ we need to determine which of the $p_k(x)$ is the largest
* It turns out this is equivalent to assigning $x$ to the class with the largest **discriminant score**

$$\delta_k(x) = x \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k)$$

--

* This **discrimiminant score** , $\delta_k(x)$, is a function of $p_k(x)$ (we took some logs and discarded terms that don't include $k$)
* $\delta_k(x)$ is a **linear** function of $x$

--

.question[
If $K = 2$, how do you think we would calculate the decision boundary?
]

---

## Discriminant functions

$$
\begin{align}
\delta_1(x) &= \delta_2(x)
\end{align}
$$

--
* Let's set $\pi_1 = \pi_2 = 0.5$

--

$$
\begin{align}
x\frac{\mu_1}{\sigma^2}-\frac{\mu_1^2}{2\sigma^2}+\log(0.5) & = x\frac{\mu_2}{\sigma^2}-\frac{\mu_2^2}{2\sigma^2}+\log(0.5)\\
x\frac{\mu_1}{\sigma^2}-x\frac{\mu_2}{\sigma^2} &= -\frac{\mu_2^2}{2\sigma^2}+\log(0.5) +\frac{\mu_1^2}{2\sigma^2}-\log(0.5)\\
x(\mu_1-\mu_2)&=\frac{\mu_1^2-\mu_2^2}{2}\\
x & = \frac{\mu_1^2-\mu_2^2}{(\mu_1-\mu_2)2}\\
x &= \frac{(\mu_1-\mu_2)(\mu_1+\mu_2)}{(\mu_1-\mu_2)2}\\
x &= \frac{\mu_1+\mu_2}{2}
\end{align}
$$

---

![](img/05/lda-2.png)

* $\mu_1 = -1.5$
* $\mu_2 = 1.5$
* $\pi_1=\pi_2=0.5$
* $\sigma^2=1$
--

* typically we don't know the **true** parameters, we just use our training data to estimate them

---

## Estimating parameters

$$\hat{\pi}_k = \frac{n_k}{n}$$
--

$$\hat{\mu}_k = \frac{1}{n_k}\sum_{i:y_i=k}x_i$$

--

$$
\begin{align}
\hat{\sigma}^2 &= \frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat{\mu_k})^2\\
&=\sum_{k=1}^K\frac{n_k-1}{n-K}\hat\sigma^2_k
\end{align}
$$

--

$$\hat{\sigma}_k^2= \frac{1}{n_k-1}\sum_{i:y_i=k}(x_i-\hat{\mu}_k)^2$$

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3


$$\hat{\pi}_k = \frac{n_k}{n}$$
```{r}
df <- tibble(x = c(-1.6, 0.2, -0.9, -2, -3, 1.9, 1.2, 2.2, 2.7, -0.5, 1.8, 3.3, 5.0, 3.4, 4.2),
             y = rep(c(1, 2, 3), each = 5))
```

```{r, echo = TRUE}
df %>%
  group_by(y) %>%
  summarise(n = n()) %>%
  mutate(pi = n / sum(n))
```

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\hat{\pi}_k = \frac{n_k}{n}$$

.pull-left[

```{r, echo = TRUE}
df %>%
  group_by(y) %>% #<<
  summarise(n = n()) %>%
  mutate(pi = n / sum(n))
```
]

.pull-right[
* `group_by()`: do calculations on groups
]
---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\hat{\pi}_k = \frac{n_k}{n}$$

.pull-left[

```{r, echo = TRUE}
df %>%
  group_by(y) %>%
  summarise(n = n()) %>% #<<
  mutate(pi = n / sum(n))
```

]

.pull-right[
* `group_by()`: do calculations on groups
* `summarise()`: reduce variables to values
]
---


## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 |5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\hat{\pi}_k = \frac{n_k}{n}$$

.pull-left[

```{r, echo = TRUE}
df %>%
  group_by(y) %>%
  summarise(n = n()) %>% 
  mutate(pi = n / sum(n)) #<<
```

]

.pull-right[
* `group_by()`: do calculations on groups
* `summarise()`: reduce variables to values
* `mutate()`: add new variables
]
---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 |5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\hat{\pi}_k = \frac{n_k}{n}$$

.pull-left[

```{r, echo = TRUE, eval = FALSE}
df %>%
  group_by(y) %>%
  summarise(n = n()) %>% 
  mutate(pi = n / sum(n)) 
```

]

.pull-right[
* `group_by()`: do calculations on groups
* `summarise()`: reduce variables to values
* `mutate()`: add new variables
]

.question[
How do we pull $\pi_k$ out into their own R object?
]

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 |5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\hat{\pi}_k = \frac{n_k}{n}$$

```{r, echo = TRUE}
df %>%
  group_by(y) %>%
  summarise(n = n()) %>% 
  mutate(pi = n / sum(n)) %>%
  pull(pi) -> pi #<<
```


.question[
How do we pull $\pi_k$ out into their own R object?
]

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 |5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\hat{\pi}_k = \frac{n_k}{n}$$

```{r, echo = TRUE}
pi
```

.question[
How do we pull $\pi_k$ out into their own R object?
]
---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\hat{\mu}_k = \frac{1}{n_k}\sum_{i:y_i=k}x_i$$

```{r, echo = TRUE}
df %>%
  group_by(y) %>%
  summarise(mu = mean(x)) #<<
```

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\hat{\mu}_k = \frac{1}{n_k}\sum_{i:y_i=k}x_i$$

```{r, echo = TRUE}
df %>%
  group_by(y) %>%
  summarise(mu = mean(x)) %>%
  pull(mu) -> mu #<<
```

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$
\begin{align}
\hat{\sigma}^2 =\sum_{k=1}^K\frac{n_k-1}{n-K}\hat\sigma^2_k
\end{align}
$$

.small[

```{r, echo = TRUE}
df %>%
  group_by(y) %>%
  summarise(var_k = var(x),
            n = n()) %>%
  mutate(v = ((n - 1) / (sum(n) - 3)) * var_k) %>%
  summarise(sigma_sq = sum(v))
```

]

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$
\begin{align}
\hat{\sigma}^2 =\sum_{k=1}^K\frac{n_k-1}{n-K}\hat\sigma^2_k
\end{align}
$$

```{r, echo = TRUE}
df %>%
  group_by(y) %>%
  summarise(var_k = var(x),
            n = n()) %>%
  mutate(v = ((n - 1) / (sum(n) - 3)) * var_k) %>%
  summarise(sigma_sq = sum(v)) %>%
  pull(sigma_sq) -> sigma_sq
```

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\delta_k(x) = x \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k)$$

* Let's predict the class for $x = 2$
```{r, echo = TRUE}
x <- 2
x * (mu / sigma_sq) - mu^2 / (2 * sigma_sq) + log(pi)
```

--
.question[
Which class should we give this point?
]
---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

$$\delta_k(x) = x \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k)$$

* Let's predict the class for $x = 6$
```{r, echo = TRUE}
x <- 6
x * (mu / sigma_sq) - mu^2 / (2 * sigma_sq) + log(pi)
```

--
.question[
Which class should we give this point?
]

---

## From the discriminant score to probabilties

We can turn $\hat{\delta}_k(x)$ into estimates for class probabilities

--

$$\hat{P}(Y=k|X=x)=\frac{e^{\hat{\delta}_k(x)}}{\sum_{l=1}^Ke^{\hat{\delta}_l(x)}}$$

--
* Classifying the largest $\hat{\delta}_k(x)$ is the same as classifying to the class with the largest $\hat{P}(Y=k|X=x)$
--

* For $K=2$:
  * classify to 2 if $\hat{P}(Y=2|X=x)\ge 0.5$
  * classify to 1 otherwise

---


## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3


$$\hat{P}(Y=k|X=x)=\frac{e^{\hat{\delta}_k(x)}}{\sum_{l=1}^Ke^{\hat{\delta}_l(x)}}$$

* Let's get the posterior probability of each class for $x = 6$

.small[
```{r, echo = TRUE}
x <- 6
d <- x * (mu / sigma_sq) - mu^2 / (2 * sigma_sq) + log(pi)
exp(d) / sum(exp(d))
```
]

---

## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

* There is a function to do this in R called `lda()` in the **MASS** package

.small[
```{r, echo = TRUE}
library(MASS) #<<
model <- lda(y ~ x, data = df)
```
]

---


## Estimating parameters (in R!)

x | -1.6| 0.2| -0.9| -2.0| -3.0| 1.9| 1.2| 2.2| 2.7| -0.5 | 1.8 | 3.3 | 5.0 | 3.4 | 4.2
--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--
y | 1 | 1 | 1| 1| 1| 2|2|2|2|2|3|3|3|3|3

* There is a function to do this in R called `lda()` in the **MASS** package

.small[
```{r, echo = TRUE}
library(MASS) 
model <- lda(y ~ x, data = df)
predict(model, newdata = data.frame(x = 6)) #<<
```
]

---

## <i class="fas fa-laptop"></i> `LDA`

- Go to the [sta-363-s20 GitHub organization](https://github.com/sta-363-s20) and search for `appex-02-lda`
- Clone this repository into RStudio Cloud
- Complete the exercises
- **Knit, Commit, Push**

---

## Linear discriminant analysis $p>1$

![](img/05/lda-p.png)

* When $p>1$ the density takes on the **multivariate normal** density

$$f(x) = \frac{1}{(2\pi)^{p/2}|\mathbf\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^T\mathbf\Sigma^{-1}(x-\mu)}$$

---

## Linear discriminant analysis $p>1$

![](img/05/lda-p.png)

* The **discriminant function** is now

$$\delta_k(x)=x^T\mathbf\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\mathbf\Sigma^{-1}\mu_k+\log\pi_k$$

* This is still a linear function!

---

## Example $p = 2$, $K = 3$

![](img/05/lda-3.png)

* Here $\pi_1 = \pi_2 = \pi_3 = 1/3$
* The dashed lines the Bayes decision boundaries
  * If they were known, they would yield the fewest misclassification errors, among all possible classifiers.
  
---

## LDA on Credit Data

| True Default (No) | True Default (Yes) | Total
-|------------------|---------------------|-----
**Predicted Default (No)** | 9644 | 252| 9895
**Predicted Default (Yes)** | 23 | 81 | 104
**Total** | 9667 | 333 | 10000

.question[
What is the misclassification rate?
]

--

* $\frac{23 + 252}{10000}$ errors - $2.75\%$ misclassification
--

.question[
Since this is **training error** what is a possible concern?
]

--
* This could be **overfit**

---

## LDA on Credit Data

| True Default (No) | True Default (Yes) | Total
-|------------------|---------------------|-----
**Predicted Default (No)** | 9644 | 252| 9895
**Predicted Default (Yes)** | 23 | 81 | 104
**Total** | 9667 | 333 | 10000


* $\frac{23 + 252}{10000}$ errors - $2.75\%$ misclassification
* ~~This could be **overfit**~~
* Since we have a **large n** and **small p** ( $n = 10,000$, $p = 4$ ) we aren't too worried about overfitting

---

## LDA on Credit Data

| True Default (No) | True Default (Yes) | Total
-|------------------|---------------------|-----
**Predicted Default (No)** | 9644 | 252| 9895
**Predicted Default (Yes)** | 23 | 81 | 104
**Total** | 9667 | 333 | 10000


* $\frac{23 + 252}{10000}$ errors - $2.75\%$ misclassification
--

.question[
What would the error rate be if we classified to the _prior_, `No` default?
]

--
* $333/10000$ - $3.33\%$

---

## LDA on Credit Data

| True Default (No) | True Default (Yes) | Total
-|------------------|---------------------|-----
**Predicted Default (No)** | 9644 | 252| 9895
**Predicted Default (Yes)** | 23 | 81 | 104
**Total** | 9667 | 333 | 10000


* $\frac{23 + 252}{10000}$ errors - $2.75\%$ misclassification
* Since we have a **large n** and **small p** ( $n = 10,000$, $p = 4$ ) we aren't too worried about overfitting
* Of the true `No`'s, we make $23/9667 = 0.2\%$ errors; of the
true `Yes`'s, we make $252/333 = 75.7\%$ errors!

---

## Types of errors

* **False positive rate**: The fraction of truly negative that are classified as positive
* **False negative rate**: The fraction of truly positive that are classified as negative

--

.question[
What is the false positive rate in the Credit Default example?
]

--
* 0.2%

--

.question[
What is the false negative rate in the Credit Default example?
]

--
* 75.7%

---

## Types of errors

* **False positive rate**: The fraction of truly negative that are classified as positive
* **False negative rate**: The fraction of truly positive that are classified as negative
* The Credit Default table was created by predicting the `Yes` class if

$$\hat{P}(\texttt{Default}|\texttt{Balance, Student})\ge 0.5$$
--
* We can change the two error rates by changing the **threshold** from 0.5 to some other number between 0 and 1

$$\hat{P}(\texttt{Default}|\texttt{Balance, Student})\ge threshold$$

---

## Varying the _threshold_

![](img/05/error.png)

* To reduce the **false negative rate** we may want the threshold to be 0.1 or less

---

## ROC

.pull-left[
![](img/05/roc.png)
]


.pull-right[

* A receiver operating characteristic (ROC) curve looks at both simultaneously
* The area under the ROC curve (AUC) is sometimes a metric for performance

]

--

.question[
Which do you think is better, higher or lower AUC?
]

---

## Let's see it in R

.small[
```{r, echo = TRUE}
library(MASS)
model <- lda(default ~ balance + student, data = Default)
```

]

* Use the `lda()` function in R from the **MASS** pacakge

---

## Let's see it in R

.small[
```{r, echo = TRUE}
library(MASS)
model <- lda(default ~ balance + student, data = Default)
predictions <- predict(model) #<<
```

]

* Use the `lda()` function in R from the `MASS` pacakge
* Get the predicted classes along with posterior probabilities using the `predict()` function

---

## Let's see it in R

.small[
```{r, echo = TRUE, eval = FALSE}
library(MASS)
model <- lda(default ~ balance + student, data = Default)
predictions <- predict(model)
Default %>%
  mutate(predicted_class = predictions$class) #<<
```
]

* Use the `lda()` function in R from the `MASS` pacakge
* Get the predicted classes along with posterior probabilities using the `predict()` function
* Add the predicted class using the `mutate()` function

---

## Let's see it in R

.small[
```{r, echo = TRUE}
library(MASS)
model <- lda(default ~ balance + student, data = Default)
predictions <- predict(model)
Default %>%
  mutate(predicted_class = predictions$class) %>%
  summarise(fpr = 
              sum(default == "No" & predicted_class == "Yes") / #<<
              sum(default == "No"), #<<
            fnr = 
              sum(default == "Yes" & predicted_class == "No") / #<<
              sum(default == "Yes")) #<<
```
]

* Use the `summarise()` function to add the false positive and false negative rates

---

## Other forms of discriminant analysis

$$P(Y=k|X=x) = \frac{\pi_kf_k(x)}{\sum_{l=1}^K\pi_lf_l(x)}$$

--

* When $f_k(x)$ are **normal** densities with the **same covariance** matrix $\mathbf\Sigma$ in each class, this is **linear discriminant analysis**
--

* When $f_k(x)$ are **normal** densities with **different covariance** matrices $\mathbf\Sigma_k$ in each class, this is **quadratic discriminant analysis**
--

* Lots of other forms are possible! 

---

## Quadratic Discriminant Analysis

<img src = "img/05/qda.png" height = 250></img>

$$\delta_k(x) = -\frac{1}{2}(x-\mu_k)^T\mathbf\Sigma_k^{-1}(x-\mu_k)+\log\pi_k$$
.question[
Why do you think this is called **quadratic** discriminant analysis?
]
--

* Because the $\mathbf\Sigma_k$ are different, the quadratic terms matter

---


## Let's see it in R

.small[
```{r, echo = TRUE}
library(MASS)
model <- qda(default ~ balance + student, data = Default) #<<
predictions <- predict(model)
```

]

* Use the `qda()` function in R from the **MASS** pacakge

---

## Logistic Regression versus LDA

* For the two-class problem ( $K=2$ ), LDA takes the form

$$\log\left(\frac{p_1(x)}{1-p_1(x)}\right)=\log\left(\frac{p_1}{p_2}\right) = c_0 + c_1x_1 + \dots+ c_px_p$$
--

* This is the same form as logistic regression
--

* The difference is in how the parameters are estimated
--

  * Logistic regression uses the conditional likelihood based on
$P(Y|X)$ (**discriminative learning**)
--

  * LDA uses the full likelihood based on $P(X,Y)$ (**generative learning**)
--

  * The results are often similar
  
---

## Summary


* Logistic regression is very popular for classification, especially when $K = 2$ 
* LDA is useful when $n$ is small, or the classes are well separated, and normality assumptions are reasonable. Also when $K > 2$
--

* See Section 4.5 in your book for some comparisons of logistic regression, LDA, and KNN.