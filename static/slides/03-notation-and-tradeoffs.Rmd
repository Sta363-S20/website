---
title: "Trade-offs: Accuracy and interpretability, bias and variance"
author: "Dr. D'Agostino McGowan"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r child = "setup.Rmd"}
```

layout: true

<div class="my-footer">
  <span>
  Dr. Lucy D'Agostino McGowan <i>adapted from slides by Hastie & Tibshirani</i>
</span>
</div> 

---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
```

## Regression and Classification

* Regression: quantitative response
* Classification: qualitative (categorical) response

---

## Regression and Classification

.question[
What would be an example of a **regression** problem?
]

* Regression: quantitative response
* Classification: qualitative (categorical) response

---


## Regression and Classification

.question[
What would be an example of a **classification** problem?
]

* Regression: quantitative response
* Classification: qualitative (categorical) response

---

## Auto data


```{r, fig.height = 2}
library(ISLR)
library(gridExtra)
p1 <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(size = 1) + geom_smooth(method = "lm") +
  theme_classic()
p2 <- ggplot(Auto, aes(weight, mpg)) + 
  geom_point(size = 1) + 
  geom_smooth(method = "lm") + 
  theme_classic()
p3 <- ggplot(Auto, aes(acceleration, mpg)) +
  geom_point(size = 1) + 
  geom_smooth(method = "lm") + 
  theme_classic()
grid.arrange(p1, p2, p3, nrow = 1)
```

Above are `mpg` vs `horsepower`, `weight`, and `acceleration`, with a blue linear-regression line fit separately to each. Can we predict `mpg` using these three?

--

Maybe we can do better using a model:

$\texttt{mpg} \approx f(\texttt{horsepower}, \texttt{weight}, \texttt{acceleration})$

---

## Notation

* `mpg` is the **response** variable, the **outcome** variable, we refer to this as $Y$
* `horsepower` is a **feature**, **input**, **predictor**, we refer to this as $X_1$
* `weight` is $X_2$
* `acceleration` is $X_3$
--

* Our **input vector** is

$$X = \begin{bmatrix} X_1 \\X_2 \\X_3\end{bmatrix}$$
--

* Our **model** is

$$Y = f(X) + \epsilon$$
* $\epsilon$ is our error

---

## Why do we care about $f(X)$?

* We can use $f(X)$ to make predictions of $Y$ for new values of $X = x$
--

* We can gain a better understanding of which components of $X = (X_1, X_2, \dots, X_p)$ are important for explaining $Y$
--

* Depending on how complex $f$ is, maybe we can understand how each component ( $X_j$ ) of $X$ affects $Y$

---

```{r, fig.height = 2}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(size = 3, color = "grey") + 
  geom_smooth(method = "loess", se = FALSE) + 
  geom_vline(xintercept = 100, color = "blue") +
  geom_point(aes(x = 100, y = 21.6), size = 3, color = "blue") +
  theme_classic()
```
 How do we choose $f(X)$? What is a good value for
$f(X)$ at any selected value of $X$, say $X = 100$? There can be many $Y$ values at $X = 100$. 
--

A good value is

$$f(100) = E(Y|X = 100)$$

--

$E(Y|X = 100)$ means **expected value** (average) of $Y$ given $X = 100$

--

This ideal $f(x) = E(Y | X = x)$ is called the **regression function**

---

## Regression function, $f(X)$

* Also works or a vector, $X$, for example,

$$f(x) = f(x_1, x_2, x_3) = E[Y | X_1 = x_1, X_2 = x_2, X_3 = x_3]$$

* This is the **optimal** predictor of $Y$ in terms of **mean-squared prediction error**
--

.definition[
$f(x) = E(Y|X = x)$ is the function that **minimizes** $E[(Y - g(X))^2 |X = x]$ over all
functions $g$ at all points $X = x$
]

--
* $\epsilon = Y - f(x)$ is the **irreducible error** 
* even if we knew $f(x)$, we would still make errors in prediction, since at each $X = x$ there is typically a distribution of possible $Y$ values

---

```{r, fig.height = 3}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(color = "grey") + 
  geom_smooth(method = "loess", se = FALSE) + 
  geom_vline(xintercept = 100, color = "blue") +
  geom_point(aes(x = 100, y = 21.6), size = 3, color = "blue") +
  theme_classic()
```

---

```{r, fig.height = 3}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(color = "grey") + 
  geom_smooth(method = "loess", se = FALSE) + 
  geom_point(data = Auto %>% filter(horsepower == 100), aes(horsepower, mpg), color = "blue") +
  theme_classic()
```

```{r, eval = FALSE}
Auto %>%
  filter(horsepower == 100) %>%
  summarise(mean(mpg))
```

.question[
Using these points, how would I calculate the **regression function**?
]

--
* Take the average! $f(100) = E[\texttt{mpg}|\texttt{horsepower} = 100] = 19.6$

---

```{r, fig.height = 3}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(color = "grey") + 
  geom_point(aes(x = 100, y = 32.9), size = 4, color = "red") + 
  geom_smooth(method = "loess", se = FALSE) + 
  geom_point(data = Auto %>% filter(horsepower == 100), aes(horsepower, mpg), color = "blue") +
  theme_classic()
```

.question[
This point has a $Y$ value of 32.9. What is $\epsilon$?
]
--

* $\epsilon = Y - f(X) = 32.9 - 19.6 = \color{red}{13.3}$

---

## The error

For any estimate, $\hat{f}(x)$, of $f(x)$, we have

$$E[(Y - \hat{f}(x))^2 | X = x] = \underbrace{[f(x) - \hat{f}(x)]^2}_{\textrm{reducible error}} + \underbrace{Var(\epsilon)}_{\textrm{irreducible error}}$$

---

## Estimating $f$

* Typically we have very few (if any!) data points at $X=x$ exactly, so we cannot compute $E[Y|X=x]$
--

* For example, what if we were interested in estimating miles per gallon when horsepower was 104.

```{r, fig.height = 1.25}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(color = "grey") + 
  geom_smooth(method = "loess", se = FALSE) + 
  geom_vline(xintercept = 104, color = "blue") +
  geom_point(aes(x = 104, y = 20.8), color = "blue") +
  theme_classic()
```
--

`r emo::ji("bulb")` We can _relax_ the definition and let

$$\hat{f}(x) = E[Y | X\in \mathcal{N}(x)]$$

---

## Estimating $f$

* Typically we have very few (if any!) data points at $X=x$ exactly, so we cannot compute $E[Y|X=x]$
* For example, what if we were interested in estimating miles per gallon when horsepower was 104.

```{r, fig.height = 1.25}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(color = "grey") + 
  geom_smooth(method = "loess", se = FALSE) + 
  geom_vline(xintercept = c(100, 108), color = "blue", lty = 2) +
  geom_vline(xintercept = 104, color = "blue") +
  geom_point(aes(x = 104, y = 20.8), color = "blue") +
  theme_classic()
```

`r emo::ji("bulb")` We can _relax_ the definition and let

$$\hat{f}(x) = E[Y | X\in \mathcal{N}(x)]$$

* Where $\mathcal{N}(x)$ is some **neighborhood** of $x$

---

## Notation pause!

<br><br><br>

$$\hat{f}(x) = \underbrace{E}_{\textrm{The expectation}}[\underbrace{Y}_{\textrm{of Y}} \underbrace{|}_{\textrm{given}} \underbrace{X\in \mathcal{N}(x)}_{\textrm{X is in the neighborhood of x}}]$$
--

.alert[
If you need a notation pause at any point during this class, please let me know! 
]