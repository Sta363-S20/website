---
title: "tidymodels"
author: "Dr. D'Agostino McGowan"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---



```{r child = "setup.Rmd"}
```

layout: true

<div class="my-footer">
<span>
Dr. Lucy D'Agostino McGowan <i> adapted from Alison Hill's Introduction to ML with the Tidyverse</i>
</span>
</div> 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidymodels)
library(broom)
library(ISLR)
library(countdown)
```

---

## <i class="fas fa-laptop"></i> `tidymodels`

- Go to the [sta-363-s20 GitHub organization](https://github.com/sta-363-s20) and search for `appex-04-tidymodels`
- Go to RStudio Pro
- rstudio.hpc.ar53.wfu.edu:8787
- pw: R2D2Star!

---

## tidymodels

.pull-left[
![](img/02/tidymodels.png)
]

.pull-right[
.center[
[tidyverse.org](https://www.tidyverse.org/)
]

- tidymodels is an opinionated collection of R packages designed for modeling and statistical analysis.
- All packages share an underlying philosophy and a common grammar.
]

---

## Step 1: Specify the model

* Pick the **model**
--

* Set the **engine**

---

## Specify the model


```{r, eval = FALSE}
linear_reg() %>%
  set_engine("lm")
```

---

## Specify the model


```{r, eval = FALSE}
linear_reg() %>%
  set_engine("glmnet")
```

---

## Specify the model


```{r, eval = FALSE}
linear_reg() %>%
  set_engine("spark")
```

---

## Specify the model


```{r, eval = FALSE}
decision_tree() %>%
  set_engine("ranger")
```

---

## Specify the model

* All available models:

https://tidymodels.github.io/parsnip/articles/articles/Models.html

---

class: inverse

## <i class="fas fa-laptop"></i> `Specify Model`

Write a pipe that creates a model that uses `lm()` to fit a linear regression using tidymodels. Save it as `lm_spec` and look at the object. What does it return?

_Hint: you'll need  https://tidymodels.github.io/parsnip/articles/articles/Models.html_

`r countdown::countdown(minutes = 2)`

---

```{r}
lm_spec <- 
  linear_reg() %>% # Pick linear regression
  set_engine(engine = "lm") # set engine
lm_spec
```

---

## Fit the data

* You can train your model using the `fit()` function

```{r}
fit(lm_spec,
    formula = mpg ~ horsepower,
    data = Auto)
```

---

class: inverse

## <i class="fas fa-laptop"></i> `Fit Model`

Fit the model:

```{r, eval = FALSE}
library(ISLR)
lm_fit <- fit(lm_spec,
              formula = mpg ~ horsepower,
              data = Auto)
lm_fit
```

Does this give the same results as

```{r, eval = FALSE}
lm(mpg ~ horsepower, data = Auto)
```

`r countdown::countdown(1, 30)`

---

```{r, echo = FALSE}
lm_fit <- fit(lm_spec,
              formula = mpg ~ horsepower,
              data = Auto)
```


## Get predictions

```{r, eval = FALSE}
lm_fit %>%
  predict(new_data = Auto)
```

--

* Still uses the `predict()` function
--

* `r emo::ji("double_exclamation_mark")` Now `new_data` has an underscore
--

* `r emo::ji("smile")` This automagically creates a data frame

---

## Get predictions

```{r}
lm_fit %>%
  predict(new_data = Auto) %>%
  bind_cols(Auto)
```

---

class: inverse

`r countdown::countdown(minutes = 1, 30)`

## <i class="fas fa-laptop"></i> `Get predictions`

Edit the code below to add the original data to the predicted data.


```{r, eval = FALSE}
mpg_pred <- lm_fit %>% 
  predict(new_data = Auto) %>% 
  ---
```

---

## Get predictions

```{r}
mpg_pred <- lm_fit %>%
  predict(new_data = Auto) %>%
  bind_cols(Auto)

mpg_pred
```

---

## Calculate the error

* Root mean square error

```{r}
mpg_pred %>%
  rmse(truth = mpg, estimate = .pred)
```

--

.question[
What is this estimate? (training error? testing error?)
]

---

## Validation set approach

```{r}
Auto_split <- initial_split(Auto, prop = 0.5)
Auto_split
```

--

* Extract the training and testing data

```{r, eval = FALSE}
training(Auto_split)
testing(Auto_split)
```

---

## Validation set approach

```{r}
Auto_train <- training(Auto_split)
```

```{r, eval = FALSE}
Auto_train
```

.small[
```{r, echo = FALSE}
as_tibble(Auto_train)
```
]


---

class: inverse

`r countdown::countdown(minutes = 4)`

## <i class="fas fa-laptop"></i> `Validation Set`

Copy the code below, fill in the blanks to fit a model on the **training** data then calculate the **test** RMSE.

```{r, eval = FALSE}
set.seed(100)
Auto_split  <- ________
Auto_train  <- ________
Auto_test   <- ________
lm_fit      <- fit(lm_spec, 
                   formula = mpg ~ horsepower, 
                   data = ________)
mpg_pred  <- ________ %>% 
  predict(new_data = ________) %>% 
  bind_cols(________)
rmse(________, truth = ________, estimate = ________)
```

---

## What about cross validation?

```{r}
Auto_cv <- vfold_cv(Auto, v = 5)
Auto_cv
```

---

## What about cross validation?

--

```{r, eval = FALSE}
fit_resamples(lm_spec,
              mpg ~ horsepower,
              resamples = Auto_cv)
```

---

## What about cross validation?

```{r}
fit_resamples(lm_spec,
              mpg ~ horsepower,
              resamples = Auto_cv)
```

---

## What about cross validation?

.question[
How do we get the metrics out? With `collect_metrics()`!
]

--

```{r}
results <- fit_resamples(lm_spec,
                         mpg ~ horsepower,
                         resamples = Auto_cv)

results %>%
  collect_metrics()
```

---


class: inverse

`r countdown::countdown(minutes = 2)`

## <i class="fas fa-laptop"></i> `K-fold cross validation`


Edit the code below to get the 5-fold cross validation error rate for the following model:

$mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2+ \epsilon$

```{r, eval = FALSE}
Auto_cv <- vfold_cv(Auto, v = 5)

results <- fit_resamples(lm_spec,
                         ----,
                         resamples = ---)

results %>%
  collect_metrics()
```

* What do you think `rsq` is?

---

## What if we wanted to do some _preprocessing_

* For the shrinkage methods we discussed it was important to _scale_ the variables

--

.question[
What does this mean?
]

--

.question[
What would happen if we _scale_ **before** doing cross-validation? Will we get different answers?
]

---

## What if we wanted to do some _preprocessing_

.small[
```{r}
Auto_scaled <- Auto %>%
  mutate(horsepower = scale(horsepower))

sd(Auto_scaled$horsepower)
```


```{r}
Auto_cv_scaled <- vfold_cv(Auto_scaled, v = 5)

map_dbl(Auto_cv_scaled$splits,
        function(x) {
          dat <- as.data.frame(x)$horsepower
          sd(dat)
        })
```

]

---

## What if we wanted to do some _preprocessing_

* `recipe()`!
--

* Using the `recipe()` function along with `step_*()` functions, we can specify _preprocessing_ steps and R will automagically apply them to each fold appropriately.
--

```{r}
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_scale(horsepower) #<<
```

--

* You can find all of the potential preprocessing **steps** here: https://tidymodels.github.io/recipes/reference/index.html

---

## Where do we plug in this recipe?

* The `recipe` gets plugged into the `fit_resamples()` function

--

```{r}
Auto_cv <- vfold_cv(Auto, v = 5)

rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_scale(horsepower)

results <- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)

results %>%
  collect_metrics()
```

---
class: inverse

`r countdown::countdown(minutes = 3)`

## <i class="fas fa-laptop"></i> `Pre-processing`

* Look through the available steps here: https://tidymodels.github.io/recipes/reference/index.html
* See if you can find a way to make `horsepower` a 3rd degree polynomial (like in the lab) 
* Edit the code below to run this model

```{r, eval = FALSE}
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_---(horsepower, ---)

results <- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)

results %>%
  collect_metrics()
```

---

## Recipe!

```{r}
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_poly(horsepower, degree = 3)

results <- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)

results %>%
  collect_metrics()
```

---

## Okay, but we wanted to look at 3 different models!

.small[
```{r, eval = FALSE}
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_poly(horsepower, degree = 1)

results <- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)
```
]

--

.small[
```{r, eval = FALSE}
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_poly(horsepower, degree = 2)

results <- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)
```
]

---

.small[
```{r, eval = FALSE}
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_poly(horsepower, degree = 3)

results <- fit_resamples(lm_spec,
                         preprocessor = rec,
                         resamples = Auto_cv)
```
]

--

* `r emo::ji("scream")` this looks like copy + pasting!

---

## tune `r emo::ji("notes")`

* First, create a recipe with pre-processing steps

```{r}
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_poly(horsepower, degree = tune()) #<<
```

--
* Notice the code above has `tune()` for the degree of the polynomial. That is the thing we want to vary!

---

##  tune `r emo::ji("notes")`

* Now we need to create a grid of potential degrees of the polynomial that we want to test
* Instead of `fit_resamples()` we are going to use `tune_grid()`

```{r tune, cache = TRUE}
grid <- expand_grid(degree = c(1, 2, 3))

results <- tune_grid(lm_spec,
                     preprocessor = rec,
                     grid = grid, #<<
                     resamples = Auto_cv)
```

---

## tune `r emo::ji("notes")`

```{r}
results %>%
  collect_metrics()
```

---

## Subset results

```{r}
results %>%
  collect_metrics() %>%
  filter(.metric == "rmse")
```

* Since this is a data frame, we can do things like filter!
--

.question[
Which would you choose?
]

---

##  tune `r emo::ji("notes")`

* We can easily tweak this process to look at more polynomial degrees by adding more numbers to our `grid` data frame.

---

class: inverse

`r countdown::countdown(minutes = 4)`

## <i class="fas fa-laptop"></i> `K-fold cross validation`

Edit the code below to look at 1-10 degree polynomials. **Bonus** Try to plot the RMSE by degree

```{r, eval = FALSE}
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_poly(---) 

grid <- expand_grid(degree = ---)

results <- tune_grid(lm_spec,
                     preprocessor = ---,
                     grid = ---,
                     resamples = ---)

results %>%
  collect_metrics()
```


---


```{r, eval = FALSE}
results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(degree, mean)) + 
  geom_point() + 
  labs(y = RMSE)
```

```{r rmse-plot, cache = TRUE, echo = FALSE}
set.seed(1)
rec <- recipe(mpg ~ horsepower, data = Auto) %>%
  step_poly(horsepower, degree = tune()) 

wrkflow <- workflow() %>%
  add_recipe(rec) %>% 
  add_model(lm_spec)

grid <- expand_grid(degree = 1:10)

results <- tune_grid(wrkflow,
                     grid = grid,
                     resamples = Auto_cv)

results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(degree, mean)) + 
  geom_point() + 
  labs(y = "RMSE")
```

---

## Ridge, Lasso, and Elastic net

* When specifying your model, you can indicate whether you would like to use ridge, lasso, or elastic net. We can write a general equation to minimize:

$$RSS + \lambda\left((1-\alpha)\sum_{i=1}^p\beta_j^2+\alpha\sum_{i=1}^p|\beta_j|\right)$$

--

```{r}
lm_spec <- linear_reg() %>%
  set_engine("glmnet") #<<
```

* First specify the engine. We'll use `glmnet`
--

* The `linear_reg()` function has two additional parameters, `penalty` and `mixture`
--

* `penalty` is $\lambda$ from our equation. 
--

* `mixture` is a number between 0 and 1 representing $\alpha$

---


## Ridge, Lasso, and Elastic net


$$RSS + \lambda\left((1-\alpha)\sum_{i=1}^p\beta_j^2+\alpha\sum_{i=1}^p|\beta_j|\right)$$


.question[
What would we set `mixture` to in order to perform Ridge regression?
]

--

.small[
```{r}
ridge_spec <- linear_reg(penalty = 100, mixture = 0) %>% #<<
  set_engine("glmnet") 
```
]

---


class: inverse

`r countdown::countdown(minutes = 2)`

## <i class="fas fa-laptop"></i> `Lasso specification`

Set up the model specification to fit a Lasso with a $\lambda$ value of 5. Call this object `lasso_spec`.


---

## Ridge, Lasso, and Elastic net


$$RSS + \lambda\left((1-\alpha)\sum_{i=1}^p\beta_j^2+\alpha\sum_{i=1}^p|\beta_j|\right)$$

.small[
```{r}
ridge_spec <- linear_reg(penalty = 100, mixture = 0) %>% #<<
  set_engine("glmnet") 
```
]

--

.small[
```{r}
lasso_spec <- linear_reg(penalty = 5, mixture = 1) %>% #<<
  set_engine("glmnet") 
```
]

--

.small[
```{r}
enet_spec <- linear_reg(penalty = 60, mixture = 0.7) %>% #<<
  set_engine("glmnet") 
```
]

---

## Motivating example

We want to predict a baseball player's `Salary` on the basis of various statistics
associated with performance in the previous year.

* We will use the `Hitters` data frame from the **ISLR** package
--

* This data frame has some missing values. For simplicity, we are going to remove all missing rows (do a **complete case** analysis) using:

.small[
```{r}
hitters <- na.omit(Hitters)
```
]

_If you do this in practice, be sure to state that explictly_.

---

class: inverse

`r countdown::countdown(minutes = 2)`

## <i class="fas fa-laptop"></i> `Baseball prediction, CV set up`

Set up the 5-fold cross validation by creating an object called `hitters_cv`

```{r, eval = FALSE}
library(ISLR)
hitters <- na.omit(Hitters)
set.seed(1)

hitters_cv <- -----(hitters, -----)
```

---

## Baseball prediction cross validation

```{r}
set.seed(1)
hitters_cv <- vfold_cv(hitters, v = 5)
```

---

## Baseball prediction preprocessing

.question[
How does regression handle categorical (nominal) predictors?
]

--

* Creates "dummy" variables

--

.small[
```{r, echo = FALSE}
iris %>%
  slice(c(1, 10, 100, 150)) %>%
  select(Species)
```

]

--

.small[
```{r, echo = FALSE}
iris %>%
  recipe() %>%
  step_dummy(Species) %>%
  prep() %>%
  juice() %>%
  select(starts_with("Species")) %>%
  slice(c(1, 10, 100, 150))
```
]


--

---

## Baseball prediction preprocessing

* The first preprocessing step we want to do is make all nominal variables **dummy variables**.
* You can do this by specifying `step_dummy()` in a `recipe`
--

* Specify the variables you want to be preprocessed by seperating them with commas:

```{r, eval = FALSE}
step_dummy(nominal_variable_1, nominal_variable_2, ...)
```

---

class: inverse

`r countdown::countdown(minutes = 2)`

## <i class="fas fa-laptop"></i> `Baseball prediction, preprocessing`

For this prediction model, we want to predict `Salary` using **all** remaining variables (`Salary ~ .`). Examine the `hitters` data. Find the **nominal variables**. Ammend the code below to create dummy variables for these.

```{r, eval = FALSE}
rec <- recipe(Salary ~ ., data = hitters) %>%
  step_dummy(------)
```

---

## Baseball prediction preprocessing

_There is an easier way!_ 

--

* Instead of looking for all nominal variables by hand, you can use `all_nominal()`.
--

```{r, eval = FALSE}
rec <- recipe(Salary ~ ., data = hitters) %>%
  step_dummy(all_nominal())
```

---

## Baseball prediction preprocessing

.question[
What pre-processing step is necessary when doing penalized regression?
]

--

* Instead of plugging each variable individually into `step_scale()`, you can use the shorthand `all_predictors()`:

.small[
```{r, eval = FALSE}
step_scale(all_predictors())
```
]


---

class: inverse

`r countdown::countdown(minutes = 2)`

## <i class="fas fa-laptop"></i> `Baseball prediction, preprocessing`

Ammend the code below to make all nominal variables dummy variables and scale all of the predictors

```{r, eval = FALSE}
rec <- recipe(Salary ~ ., data = hitters) %>%
  step_-----(------) %>%
  step_-----(------)
```

---

## Baseball prediction preprocessing

```{r}
rec <- recipe(Salary ~ ., data = hitters) %>%
  step_dummy(all_nominal()) %>%
  step_scale(all_predictors())
```

---


class: inverse

`r countdown::countdown(minutes = 2)`

## <i class="fas fa-laptop"></i> `Baseball prediction, lasso`

Using the recipe you've created `rec`, the cross validation data `hitters_cv`, and the `lasso_spec` fit the resampled data. Output the `rmse` and `rsq` for this model. 

```{r, eval = FALSE}
results <- fit_resamples(lasso_spec,
                         preprocessor = ---,
                         resamples = ---)

results %>%
  ---
```

---

## Ridge, Lasso, and Elastic Net

$$RSS + \lambda\left((1-\alpha)\sum_{i=1}^p\beta_j^2+\alpha\sum_{i=1}^p|\beta_j|\right)$$

.question[
What if I don't want to specify these parameters? How can I specify that I want to use cross validation to choose?
]

--

* `r emo::ji("notes")`

--

```{r, eval = FALSE}
penalized_spec <- 
  linear_reg(penalty = tune(),
             mixture = tune()) %>%
  set_engine("glmnet")

grid <- expand_grid(penalty = seq(1, 1000, by = 100),
                         mixture = seq(0, 1, by = 0.2))
```

---

class: inverse

`r countdown::countdown(minutes = 5)`

## <i class="fas fa-laptop"></i> `Baseball prediction, tuned penalization`

Create a new models specification called `penalized_spec` that tunes the penalty and mixture parameters. Create a grid of potential parameters, with a penalty term that varies from 0 to 100 by 10, and a mixture term that varies from 0 to 1 by 0.1. Using the recipe you've created `rec`and the cross validation data `hitters_cv` fit this model using `tune_grid`. Output the `rmse` and `rsq` for this model. 

.small[
```{r, eval = FALSE}
penalized_spec <- linear_reg(penalty = ---,
                             mixture = ---) %>%
                  set_engine("glmnet")

grid <- expand_grid(penalty = ---,
                    mixture = ---)

results <- tune_grid(penalized_spec,
                     preprocessor = ---,
                     grid = ---,
                     resamples = ---)

results %>%
  ---
```
]

---

## Baseball prediction, tuned penalization


```{r penalized, cache = TRUE}
penalized_spec <- 
  linear_reg(penalty = tune(),
             mixture = tune()) %>%
  set_engine("glmnet")

grid <- expand_grid(penalty = seq(0, 100, by = 10),
                         mixture = seq(0, 1, by = 0.1))

results <- tune_grid(penalized_spec,
                     rec,
                     grid = grid,
                     resamples = hitters_cv)
```

---


class: inverse

`r countdown::countdown(minutes = 2)`

## <i class="fas fa-laptop"></i> `Baseball prediction, choosing penalty and mixture`

Examine the results. Filter the metrics to only include `rmse` and arrange them in order of mean root mean squared error (to choose the smallest). Hint look at `?arrange` if you aren't sure how to sort.

```{r, eval = FALSE}
results %>%
  collect_metrics() %>%
  filter(---) %>%
  arrange(---)
```

---

## Baseball prediction tuning

```{r}
results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean)
```



